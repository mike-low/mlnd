{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Lambda\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import theano, os, h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# Paths to image data\n",
    "training_data_dir = \"../../data/train\"\n",
    "validation_data_dir = \"../../data/validation\"\n",
    "\n",
    "# Paths to network weights\n",
    "vgg16_weights_path = '../vgg16_weights.h5' # Not in GitHub, as it's too large\n",
    "vgg16_new_top_layers_weights_path = '../vgg16_new_top_layers_weights.h5'\n",
    "vgg16_fine_tune_weights_path = '../vgg_fine_tune.h5'\n",
    "\n",
    "# Training parameters\n",
    "img_width, img_height = 224, 224\n",
    "number_of_training_samples = 23000\n",
    "number_of_validation_samples = 2000\n",
    "number_of_epochs = 20\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The VGG16 ConvNet\n",
    "\n",
    "Define the VGG16 architecture to use (from https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3), without the final dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vgg16():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "  \n",
    "    # Final fully connected layers omitted, so we can use our own instead\n",
    "    # model.add(Flatten())\n",
    "    # model.add(Dense(4096, activation='relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    # model.add(Dense(4096, activation='relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    # model.add(Dense(1000, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded weights of the VGG16 network are for the full model, but we're only interested in the convolutional and pooling layers. So only load the weights for those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_vgg16()\n",
    "\n",
    "# Load the weights only up until the fully connected layers \n",
    "# (https://gist.github.com/fchollet/f35fbc80e066a49d65f1688a7e99f069)\n",
    "f = h5py.File(vgg16_weights_path)\n",
    "for k in range(f.attrs['nb_layers']):\n",
    "    if k >= len(model.layers):\n",
    "        # we don't look at the last (fully-connected) layers in the savefile\n",
    "        break\n",
    "    g = f['layer_{}'.format(k)]\n",
    "    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "    model.layers[k].set_weights(weights)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the new top layers we want to use for the model. This needs to be trained first before being combined with the pre-trained VGG model.\n",
    "\n",
    "As this model is the same model created and trained in the 'VGG16 new top layers' approach, we can solve this by just loading in those weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_layers_model = Sequential()\n",
    "top_layers_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "top_layers_model.add(Dense(256, activation='relu'))\n",
    "top_layers_model.add(Dropout(0.5))\n",
    "top_layers_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "top_layers_model.load_weights(vgg16_new_top_layers_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine our new final layers into the VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(top_layers_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "By fine-tuning, we want to adjust the existing weights based on the extra information gained from our training data.\n",
    "\n",
    "However, the lower level layers in a ConvNet represent very general, abstract features and shapes. These are not likely to change with a few thousand more images, given the existing weights were built from the millions of ImageNet images.\n",
    "\n",
    "The higher up the layer, the more specific the feature it represents. In this case, the highest level layers may well benefit from our extra training data. \n",
    "\n",
    "To deal with this, we can 'freeze' the weights of the lower layers, while keeping the highest level layers still trainable. This has the side benefit of also being much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lower_layers = model.layers[:26]\n",
    "for layer in lower_layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, prepare the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_data_generator = train_datagen.flow_from_directory(directory=training_data_dir, \n",
    "                                                            target_size=(img_width, img_height),\n",
    "                                                            batch_size=batch_size, class_mode='binary')\n",
    "validation_data_generator = val_datagen.flow_from_directory(directory=validation_data_dir, \n",
    "                                                            target_size=(img_width, img_height),\n",
    "                                                            batch_size=batch_size, class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, compile and run the fine-tune model. \n",
    "\n",
    "Using a slow learning rate is beneficial here, as the model is already pre-trained and the fine-tuning should only minimally adjust the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "23000/23000 [==============================] - 672s - loss: 0.0722 - acc: 0.9756 - val_loss: 0.1681 - val_acc: 0.9470\n",
      "Epoch 2/20\n",
      "23000/23000 [==============================] - 671s - loss: 0.0547 - acc: 0.9821 - val_loss: 0.2064 - val_acc: 0.9430\n",
      "Epoch 3/20\n",
      "23000/23000 [==============================] - 671s - loss: 0.0381 - acc: 0.9870 - val_loss: 0.2733 - val_acc: 0.9435\n",
      "Epoch 4/20\n",
      "23000/23000 [==============================] - 671s - loss: 0.0334 - acc: 0.9891 - val_loss: 0.1993 - val_acc: 0.9425\n",
      "Epoch 5/20\n",
      "23000/23000 [==============================] - 671s - loss: 0.0311 - acc: 0.9892 - val_loss: 0.1969 - val_acc: 0.9550\n",
      "Epoch 6/20\n",
      "23000/23000 [==============================] - 671s - loss: 0.0278 - acc: 0.9913 - val_loss: 0.1738 - val_acc: 0.9560\n",
      "Epoch 7/20\n",
      "23000/23000 [==============================] - 671s - loss: 0.0247 - acc: 0.9923 - val_loss: 0.2533 - val_acc: 0.9540\n",
      "Epoch 8/20\n",
      "23000/23000 [==============================] - 671s - loss: 0.0219 - acc: 0.9928 - val_loss: 0.1653 - val_acc: 0.9530\n",
      "Epoch 9/20\n",
      "23000/23000 [==============================] - 671s - loss: 0.0232 - acc: 0.9920 - val_loss: 0.2476 - val_acc: 0.9505\n",
      "Epoch 10/20\n",
      "23000/23000 [==============================] - 668s - loss: 0.0179 - acc: 0.9948 - val_loss: 0.1557 - val_acc: 0.9595\n",
      "Epoch 11/20\n",
      "23000/23000 [==============================] - 667s - loss: 0.0188 - acc: 0.9945 - val_loss: 0.2375 - val_acc: 0.9520\n",
      "Epoch 12/20\n",
      "23000/23000 [==============================] - 668s - loss: 0.0182 - acc: 0.9947 - val_loss: 0.2115 - val_acc: 0.9555\n",
      "Epoch 13/20\n",
      "23000/23000 [==============================] - 668s - loss: 0.0145 - acc: 0.9956 - val_loss: 0.3279 - val_acc: 0.9525\n",
      "Epoch 14/20\n",
      "23000/23000 [==============================] - 668s - loss: 0.0140 - acc: 0.9960 - val_loss: 0.2822 - val_acc: 0.9545\n",
      "Epoch 15/20\n",
      "23000/23000 [==============================] - 668s - loss: 0.0134 - acc: 0.9960 - val_loss: 0.2750 - val_acc: 0.9510\n",
      "Epoch 16/20\n",
      "23000/23000 [==============================] - 668s - loss: 0.0116 - acc: 0.9962 - val_loss: 0.2905 - val_acc: 0.9575\n",
      "Epoch 17/20\n",
      "23000/23000 [==============================] - 669s - loss: 0.0133 - acc: 0.9960 - val_loss: 0.2538 - val_acc: 0.9535\n",
      "Epoch 18/20\n",
      "23000/23000 [==============================] - 669s - loss: 0.0123 - acc: 0.9963 - val_loss: 0.2363 - val_acc: 0.9560\n",
      "Epoch 19/20\n",
      "23000/23000 [==============================] - 669s - loss: 0.0113 - acc: 0.9967 - val_loss: 0.2282 - val_acc: 0.9575\n",
      "Epoch 20/20\n",
      "23000/23000 [==============================] - 669s - loss: 0.0099 - acc: 0.9970 - val_loss: 0.3020 - val_acc: 0.9580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f699c6bae50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        training_data_generator,\n",
    "        samples_per_epoch=number_of_training_samples,\n",
    "        nb_epoch=number_of_epochs,\n",
    "        validation_data=validation_data_generator,\n",
    "        nb_val_samples=number_of_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(vgg16_fine_tune_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize how well the classifier did, lets look at the four possible outcomes.\n",
    "\n",
    "- Predicted cat, was a cat\n",
    "- Predicted cat, was a dog\n",
    "- Predicted dog, was a cat\n",
    "- Predicted dog, was a dog\n",
    "\n",
    "To do this, we'll need to get an array of all our predicted labels and compare that against the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(vgg16_fine_tune_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# As the generator only provides the actual classes in directory order, we need to make\n",
    "# sure the predicted classes also come out in the same order. Therefore, shuffle must be False\n",
    "visualise_validation_data_generator = val_datagen.flow_from_directory(directory=validation_data_dir, \n",
    "                                                            target_size=(img_width, img_height),\n",
    "                                                            batch_size=batch_size, class_mode='binary', \n",
    "                                                            shuffle=False)\n",
    "\n",
    "# Get predictions for all validation data\n",
    "prediction_probablities = model.predict_generator(generator=visualise_validation_data_generator, \n",
    "                                                  val_samples=number_of_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine to single array\n",
    "prediction_probablities_array = prediction_probablities[:,0]\n",
    "\n",
    "# Round them to definite 1 or 0\n",
    "predicted_labels = np.round(prediction_probablities_array).astype(np.int64)\n",
    "actual_labels = visualise_validation_data_generator.classes.astype(np.int64)[:number_of_validation_samples]\n",
    "image_filenames = visualise_validation_data_generator.filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having got all the data needed, now we just make a couple of helper methods to use when displaying the images..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML \n",
    "\n",
    "def make_html_img(path):\n",
    "     return '<img src=\"{}\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/>'.format(path)\n",
    "\n",
    "def show_random_images(image_indexes, limit=5):\n",
    "    random_indexes = np.random.permutation(image_indexes)\n",
    "    html = \"\"\n",
    "    for index in random_indexes[:limit]:\n",
    "        image_path = os.path.join(validation_data_dir, image_filenames[index])\n",
    "        html += make_html_img(image_path)\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted cat, was a cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "935 images\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../../data/validation/cats/cat.116.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/cats/cat.839.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/cats/cat.286.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/cats/cat.676.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/cats/cat.273.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_cat_correctly = np.where((predicted_labels==actual_labels) & (predicted_labels == 0))[0]\n",
    "print \"{} images\".format(len(predicted_cat_correctly))\n",
    "show_random_images(predicted_cat_correctly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Predicted cat, was a dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 images\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../../data/validation/dogs/dog.361.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/dogs/dog.233.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/dogs/dog.758.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/dogs/dog.619.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/dogs/dog.501.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/dogs/dog.395.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/dogs/dog.520.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/dogs/dog.296.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/dogs/dog.254.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/dogs/dog.941.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_cat_incorrectly = np.where((predicted_labels!=actual_labels) & (predicted_labels == 0))[0]\n",
    "print \"{} images\".format(len(predicted_cat_incorrectly))\n",
    "show_random_images(predicted_cat_incorrectly, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted dog, was a dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "981 images\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../../data/validation/dogs/dog.635.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/dogs/dog.817.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/dogs/dog.409.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/dogs/dog.311.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/dogs/dog.56.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_dog_correctly = np.where((predicted_labels==actual_labels) & (predicted_labels == 1))[0]\n",
    "print \"{} images\".format(len(predicted_dog_correctly))\n",
    "show_random_images(predicted_dog_correctly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted dog, was a cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 images\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"../../data/validation/cats/cat.335.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/cats/cat.692.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/cats/cat.578.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/cats/cat.472.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/cats/cat.171.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/cats/cat.59.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/cats/cat.922.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/cats/cat.859.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/cats/cat.412.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/><img src=\"../../data/validation/cats/cat.235.jpg\" style=\"display:inline-block;margin:10px 1px; vertical-align:top; max-width:19%\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_dog_incorrectly = np.where((predicted_labels!=actual_labels) & (predicted_labels == 1))[0]\n",
    "print \"{} images\".format(len(predicted_dog_incorrectly))\n",
    "show_random_images(predicted_dog_incorrectly, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
